{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96530dd6",
   "metadata": {},
   "source": [
    "## [1] 데이터 준비\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960e03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsData = fetch_20newsgroups(subset = 'all', remove=('headers', 'footer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107dde2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c63daca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData20 = newsData['data']\n",
    "type(newsData20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a97489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsData20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa197f71",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(newsData20[0]))\n",
    "print(newsData20[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b75041",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In article <1993Apr19.034517.12820@julian.uwo.ca>, wlsmith@valve.heart.rri.uwo.ca (Wayne Smith) writes:\n",
      "> In article <RICHK.93Apr15075248@gozer.grebyn.com> richk@grebyn.com (Richard Krehbiel) writes:\n",
      "> >>     Can anyone explain in fairly simple terms why, if I get OS/2, I might \n",
      "> >>   need an SCSI controler rather than an IDE.  Will performance suffer that\n",
      "> >>   much?  For a 200MB or so drive?  If I don't have a tape drive or CD-ROM?\n",
      "> >>   Any help would be appreciated.\n",
      "> \n",
      "> >So, when you've got multi-tasking, you want to increase performance by\n",
      "> >increasing the amount of overlapping you do.\n",
      "> >\n",
      "> >One way is with DMA or bus mastering.  Either of these make it\n",
      "> >possible for I/O devices to move their data into and out of memory\n",
      "> >without interrupting the CPU.  The alternative is for the CPU to move\n",
      "> >the data.  There are several SCSI interface cards that allow DMA and\n",
      "> >bus mastering.\n",
      ">  ^^^^^^^^^^^^\n",
      "> How do you do bus-mastering on the ISA bus?\n",
      "> \n",
      "> >IDE, however, is defined by the standard AT interface\n",
      "> >created for the IBM PC AT, which requires the CPU to move all the data\n",
      "> >bytes, with no DMA.\n",
      "> \n",
      "> If we're talking ISA (AT) bus here, then you can only have 1 DMA channel\n",
      "> active at any one time, presumably transferring data from a single device.\n",
      "> So even though you can have at least 7 devices on a SCSI bus, explain how\n",
      "> all 7 of those devices can to DMA transfers through a single SCSI card\n",
      "> to the ISA-AT bus at the same time.\n",
      "\n",
      "Think!\n",
      "\n",
      "It's the SCSI card doing the DMA transfers NOT the disks...\n",
      "\n",
      "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
      "it is attached when it wants to.\n",
      "\n",
      "An important feature of SCSI is the ability to detach a device. This frees the\n",
      "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
      "start transfers on several devices. While each device is seeking the data the\n",
      "bus is free for other commands and data transfers. When the devices are\n",
      "ready to transfer the data they can aquire the bus and send the data.\n",
      "\n",
      "On an IDE bus when you start a transfer the bus is busy until the disk has seeked\n",
      "the data and transfered it. This is typically a 10-20ms second lock out for other\n",
      "processes wanting the bus irrespective of transfer time.\n",
      "\n",
      "> \n",
      "> Also, I'm still trying to track down a copy of IBM's AT reference book,\n",
      "> but from their PC technical manual (page 2-93):\n",
      "> \n",
      "> \"The (FDD) adapter is buffered on the I.O bus and uses the System Board\n",
      "> direct memory access (DMA) for record data transfers.\"\n",
      "> I expect to see something similar for the PC-AT HDD adapter.  \n",
      "> So the lowly low-density original PC FDD card used DMA and the PC-AT\n",
      "> HDD controller doesn't!?!?  That makes real sense.\n",
      "-- \n",
      "-- -----------------------------------------------------------------------------\n",
      "Guy Dawson - Hoskyns Group Plc.\n",
      "        guyd@hoskyns.co.uk  Tel Hoskyns UK     -  71 251 2128\n",
      "        guyd@austin.ibm.com Tel IBM Austin USA - 512 838 3377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsData20[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de7b282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타겟 데이터 확인\n",
    "target = newsData['target']\n",
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ec1ddd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target => 20개\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "target => [10  3 17 ...  3  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(f'target => {len(newsData[\"target_names\"])}개')\n",
    "for name in newsData['target_names']:\n",
    "    print(name)\n",
    "print(f'target => {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c76d7",
   "metadata": {},
   "source": [
    "# @@@ 텍스트 전처리(Text preprocessing) @@@"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd948f9e",
   "metadata": {},
   "source": [
    "## [2] 데이터 전처리\n",
    "---\n",
    "- (1) 수집 데이터 기반 단어사전 생성\n",
    "- (2) 텍스트 데이터 -> 수치 데이터 변환\n",
    "- (3) 데이터 길이 결정\n",
    "- (4) 이진 정수화 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed08216",
   "metadata": {},
   "source": [
    "[ 필요한 처리 ]\n",
    "---\n",
    "- 불필요한 문자 혹은 숫자 그리고 특수기호들을 제거 -> 정규식\n",
    "- 불용어 제거\n",
    "- 대 소문자 통일화\n",
    "- 표제어 추출 (Lemmatization)\n",
    "- 어간 추출 (Stemming)\n",
    "- HTML 태그 제거 등.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b013906",
   "metadata": {},
   "source": [
    "### [2-1] 단어사전 생성\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75eacef",
   "metadata": {},
   "source": [
    "## 1) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f1cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# 해당 데이터 셋에서 각 문장들을 하나의 리스트에 담기 (문장 합치기)\n",
    "textAll = []\n",
    "for n in range(len(newsData20)):\n",
    "    textAll.append(newsData20[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da4628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 text에서 문장을 하나씩 추출\n",
    "for num in range(len(textAll)):    \n",
    "    total_token = []\n",
    "\n",
    "    # 문장 단위로 추출해서 토큰화 (리스트에 담아줌)\n",
    "    sentResult = sent_tokenize(textAll[num])\n",
    "    \n",
    "    # 리스트에서 문장을 꺼내 단어 단위로 토큰화\n",
    "    for snt in sentResult:\n",
    "        # print(f' snt => {snt}')\n",
    "        wordResult = word_tokenize(snt)\n",
    "        total_token.append(wordResult)\n",
    "        # print(f' snt => {sentResult}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae7ad59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['After',\n",
       "  'a',\n",
       "  'tip',\n",
       "  'from',\n",
       "  'Gary',\n",
       "  'Crum',\n",
       "  '(',\n",
       "  'crum',\n",
       "  '@',\n",
       "  'fcom.cc.utah.edu',\n",
       "  ')',\n",
       "  'I',\n",
       "  'got',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Phone',\n",
       "  'with',\n",
       "  '``',\n",
       "  'Pontiac',\n",
       "  'Systems',\n",
       "  \"''\",\n",
       "  'or',\n",
       "  '``',\n",
       "  'Pontaic',\n",
       "  'Customer',\n",
       "  'Service',\n",
       "  \"''\",\n",
       "  'or',\n",
       "  'whatever',\n",
       "  ',',\n",
       "  'and',\n",
       "  'inquired',\n",
       "  'about',\n",
       "  'a',\n",
       "  'rumoured',\n",
       "  'Production',\n",
       "  'Hold',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Formula',\n",
       "  'Firebird',\n",
       "  'and',\n",
       "  'Trans',\n",
       "  'Am',\n",
       "  '.'],\n",
       " ['BTW',\n",
       "  ',',\n",
       "  'Talking',\n",
       "  'with',\n",
       "  'the',\n",
       "  'dealer',\n",
       "  'I',\n",
       "  'bought',\n",
       "  'the',\n",
       "  'car',\n",
       "  'from',\n",
       "  'got',\n",
       "  'me',\n",
       "  'nowhere',\n",
       "  '.'],\n",
       " ['After',\n",
       "  'being',\n",
       "  'routed',\n",
       "  'to',\n",
       "  'a',\n",
       "  '``',\n",
       "  'Firebird',\n",
       "  'Specialist',\n",
       "  \"''\",\n",
       "  ',',\n",
       "  'I',\n",
       "  'was',\n",
       "  'able',\n",
       "  'to',\n",
       "  'confirm',\n",
       "  'that',\n",
       "  'this',\n",
       "  'is',\n",
       "  'in',\n",
       "  'fact',\n",
       "  'the',\n",
       "  'case',\n",
       "  '.'],\n",
       " ['At',\n",
       "  'first',\n",
       "  ',',\n",
       "  'there',\n",
       "  'was',\n",
       "  'some',\n",
       "  'problem',\n",
       "  'with',\n",
       "  'the',\n",
       "  '3:23',\n",
       "  'performance',\n",
       "  'axle',\n",
       "  'ratio',\n",
       "  '.'],\n",
       " ['She',\n",
       "  'would',\n",
       "  \"n't\",\n",
       "  'go',\n",
       "  'into',\n",
       "  'any',\n",
       "  'details',\n",
       "  ',',\n",
       "  'so',\n",
       "  'I',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'know',\n",
       "  'if',\n",
       "  'there',\n",
       "  'were',\n",
       "  'some',\n",
       "  'shipped',\n",
       "  'that',\n",
       "  'had',\n",
       "  'problems',\n",
       "  ',',\n",
       "  'or',\n",
       "  'if',\n",
       "  'production',\n",
       "  'was',\n",
       "  'held',\n",
       "  'up',\n",
       "  'because',\n",
       "  'they',\n",
       "  'simply',\n",
       "  'did',\n",
       "  \"n't\",\n",
       "  'have',\n",
       "  'the',\n",
       "  'proper',\n",
       "  'parts',\n",
       "  'from',\n",
       "  'the',\n",
       "  'supplier',\n",
       "  '.'],\n",
       " ['As',\n",
       "  'I',\n",
       "  'say',\n",
       "  ',',\n",
       "  'she',\n",
       "  'was',\n",
       "  'pretty',\n",
       "  'vague',\n",
       "  'on',\n",
       "  'that',\n",
       "  ',',\n",
       "  'so',\n",
       "  'if',\n",
       "  'anyone',\n",
       "  'else',\n",
       "  'knows',\n",
       "  'anything',\n",
       "  'about',\n",
       "  'this',\n",
       "  ',',\n",
       "  'feel',\n",
       "  'free',\n",
       "  'to',\n",
       "  'respond',\n",
       "  '.'],\n",
       " ['Supposedly', ',', 'this', 'problem', 'is', 'now', 'solved', '.'],\n",
       " ['Second',\n",
       "  ',',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'definate',\n",
       "  'shortage',\n",
       "  'of',\n",
       "  'parts',\n",
       "  'that',\n",
       "  'is',\n",
       "  'somehow',\n",
       "  'related',\n",
       "  'to',\n",
       "  'the',\n",
       "  'six-speed',\n",
       "  'Manual',\n",
       "  'transmission',\n",
       "  '.'],\n",
       " ['So',\n",
       "  'as',\n",
       "  'of',\n",
       "  'this',\n",
       "  'posting',\n",
       "  ',',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'production',\n",
       "  'hold',\n",
       "  'on',\n",
       "  'these',\n",
       "  'cars',\n",
       "  '.'],\n",
       " ['She',\n",
       "  'claimed',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'delay',\n",
       "  'was',\n",
       "  'not',\n",
       "  'wanting',\n",
       "  'to',\n",
       "  'use',\n",
       "  'inferior',\n",
       "  'quality',\n",
       "  'parts',\n",
       "  'for',\n",
       "  'the',\n",
       "  'car',\n",
       "  ',',\n",
       "  'and',\n",
       "  'therefore',\n",
       "  'having',\n",
       "  'to',\n",
       "  'wait',\n",
       "  'for',\n",
       "  'the',\n",
       "  'right',\n",
       "  'high',\n",
       "  'quality',\n",
       "  'parts',\n",
       "  '...'],\n",
       " ['I',\n",
       "  \"'m\",\n",
       "  'not',\n",
       "  'positive',\n",
       "  'that',\n",
       "  'this',\n",
       "  'applies',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Camaro',\n",
       "  'as',\n",
       "  'well',\n",
       "  ',',\n",
       "  'but',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'guessing',\n",
       "  'it',\n",
       "  'would',\n",
       "  '.'],\n",
       " ['Can', 'anyone', 'else', 'shed', 'some', 'light', 'on', 'this', '?'],\n",
       " ['Chris',\n",
       "  'S.',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  'Chris',\n",
       "  'Silvester',\n",
       "  '|',\n",
       "  '``',\n",
       "  'Any',\n",
       "  'man',\n",
       "  'capable',\n",
       "  'of',\n",
       "  'getting',\n",
       "  'himself',\n",
       "  'elected',\n",
       "  'President',\n",
       "  'chriss',\n",
       "  '@',\n",
       "  'sam.amgen.com',\n",
       "  '|',\n",
       "  'should',\n",
       "  'by',\n",
       "  'no',\n",
       "  'means',\n",
       "  'be',\n",
       "  'allowed',\n",
       "  'to',\n",
       "  'do',\n",
       "  'the',\n",
       "  'job',\n",
       "  \"''\",\n",
       "  'chriss',\n",
       "  '@',\n",
       "  'netcom.com',\n",
       "  '|',\n",
       "  '-',\n",
       "  'Douglas',\n",
       "  'Adams',\n",
       "  ',',\n",
       "  'The',\n",
       "  'Hitchhiker',\n",
       "  \"'s\",\n",
       "  'Guide',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Galaxy',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--',\n",
       "  '--']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b8441",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03fe8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅(part-of-speech tagging)\n",
    "# : 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분하는 것\n",
    "# ex) 부사 '못'은 동작동사를 수식할 수 없는 등의 문법적인 요소가 있는 것도 있으므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8eb0f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "print(type(tokenized_sentence))\n",
    "print(type(total_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10eb3a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 : [('After', 'IN'), ('a', 'DT'), ('tip', 'NN'), ('from', 'IN'), ('Gary', 'NNP'), ('Crum', 'NNP'), ('(', '('), ('crum', 'JJ'), ('@', 'NNP'), ('fcom.cc.utah.edu', 'NN'), (')', ')'), ('I', 'PRP'), ('got', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('Phone', 'NN'), ('with', 'IN'), ('``', '``'), ('Pontiac', 'JJ'), ('Systems', 'NNPS'), (\"''\", \"''\"), ('or', 'CC'), ('``', '``'), ('Pontaic', 'NNP'), ('Customer', 'NNP'), ('Service', 'NNP'), (\"''\", \"''\"), ('or', 'CC'), ('whatever', 'VB'), (',', ','), ('and', 'CC'), ('inquired', 'VBD'), ('about', 'IN'), ('a', 'DT'), ('rumoured', 'JJ'), ('Production', 'NNP'), ('Hold', 'NNP'), ('on', 'IN'), ('the', 'DT'), ('Formula', 'NNP'), ('Firebird', 'NNP'), ('and', 'CC'), ('Trans', 'NNP'), ('Am', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# NLTK 영어 토큰화 실습\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "print('품사 태깅 :',pos_tag(total_token[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68cdcef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils.', 'Actually,\\nI am  bit puzzled too and a bit relieved.', \"However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens.\", 'Man, they\\nare killing those Devils worse than I thought.', 'Jagr just showed you why\\nhe is much better than his regular season stats.', 'He is also a lot\\nfo fun to watch in the playoffs.', 'Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway.', 'I was very disappointed not to see the Islanders lose the final\\nregular season game.', 'PENS RULE!!', '!']\n"
     ]
    }
   ],
   "source": [
    "# 1) 불필요한 문자 혹은 숫자 그리고 특수기호들을 제거 -> 정규식\n",
    "# 어포스트로피 관련\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# - 케라스에서 제공하는 [ text_to_word_sequence ]의 경우,\n",
    "# i) 모든 알파벳을 소문자로 바꿈\n",
    "# ii) 마침표나 컴마, 느낌표 등의 구두점을 제거\n",
    "# iii) don't나 jone's와 같은 경우 아포스트로피는 보존\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "print('문장 토큰화1 :',sent_tokenize(newsData20[0]))\n",
    "# - NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83921911",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569bd51",
   "metadata": {},
   "source": [
    "## 2) 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3584caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제 작업에서 제거해야하는 노이즈 데이터(noise data)\n",
    "# - 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미\n",
    "# - 덧붙여, 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터\n",
    "\n",
    "# 2-1) 불용어 제거\n",
    "# 불용어(stopword) :  유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰\n",
    "# - 큰 의미가 없다 : 라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73b70c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 영어 ]\n",
    "# 불용어는 개발자가 직접 정의할 수도 있음\n",
    "# (1) NLTK에서 불용어 확인하기(179개)\n",
    "# (2) NLTK를 통해서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c55847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 확인\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca55874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example)   # 단어 토큰화\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de171d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 한글 ] ------ 주제에 맞는 것들을 담아서\n",
    "# 간단하게 토큰화 후에 조사, 접속사 등을 제거하는 방법\n",
    "# - 조사나 접속사와 같은 단어들뿐만 아니라\n",
    "# - 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생기기도\n",
    "#  => 결국에는 사용자가 직접 불용어 사전을 만들게 되는 경우가 많음\n",
    "\n",
    "# 직접 불용어를 정의해보고,\n",
    "# 주어진 문장으로부터 사용자가 정의한 불용어 사전으로부터 불용어를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef5bc34c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 :\n",
      "['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "\n",
      "불용어 제거 후 :\n",
      "['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거 \n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens, sep='\\n')\n",
    "print()\n",
    "print('불용어 제거 후 :',result, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2645ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2) 등장 빈도가 적은 단어 제거\n",
    "# - 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존재\n",
    "# - 직관적으로 분류에 거의 도움이 되지 않을 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d917684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3) 길이가 짧은 단어 제거 (아래의 예시 참고)\n",
    "# - 영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도\n",
    "# - 어느정도 자연어 처리에서 크게 의미가 없는 단어들을 제거하는 효과를 볼 수 있다고 알려져 있음\n",
    "# - 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하는 효과도 있음\n",
    "# - 덧붙여, 길이1 : 관사 'a'나 주어 'I'도 함께 제거\n",
    "# - 길이2 : in, by 등 전치사도 제거\n",
    "# - 길이3 : car, dog같은 명사 단어가 지워질 수 있으니 nope! (고민필요)\n",
    "\n",
    "# - but, 한국어에서는 길이가 짧은 단어라고 삭제하는 이런 방법이 크게 유효하지 않을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-4) 정규 표현식 (제거 방법)\n",
    "# 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면,\n",
    "# 정규 표현식을 통해서 이를 제거할 수 있는 경우가 많음\n",
    "# 정규 표현식은 이러한 코퍼스 내에 계속해서 등장하는 글자들을 규칙에 기반,\n",
    "# 한 번에 제거하는 방식으로서 매우 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c629b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bad9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffa2e0",
   "metadata": {},
   "source": [
    "## 3) 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f6ca7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1) 어간 추출 및 표제어 추출\n",
    "# 목적 : 하나의 단어로 일반화시킬 수 있다면, 하나의 단어로 일반화 -> 문서 내의 단어 수를 줄이자\n",
    "# -> 표기가 다르지만, 뜻은 같은 것들을 통합하는 과정\n",
    "# -> 어간 추출(stemming)과 표제어 추출(lemmatizaiton)\n",
    "\n",
    "# 이러한 방법들은 단어의 빈도수를 기반으로 문제를 풀고자 하는,\n",
    "# 뒤에서 학습하게 될 BoW(Bag of Words) 표현을 사용하는 자연어 처리 문제에서 주로 사용\n",
    "\n",
    "# 자연어 처리에서 전처리,\n",
    "# 더 정확히는 정규화의 지향점은 언제나 갖고 있는 코퍼스로부터 복잡성을 줄이는 일임!!\n",
    "\n",
    "# 이미 알려진 알고리즘을 사용할 때는,\n",
    "# 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abf5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 표제어 추출 ]\n",
    "# 표제어(Lemma) : 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미\n",
    "# => 표제어 추출은 단어들로부터 표제어를 찾아가는 과정\n",
    "\n",
    "# NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원 (아래 예시)\n",
    "# - 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있음\n",
    "# - WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있습니다.\n",
    "#  -> 즉, dies와 watched, has가 문장에서 동사로 쓰였다는 것을 알려준다면\n",
    "#  -> 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 됩니다.\n",
    "# ex) 아래 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebd9b9fc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['After', 'a', 'tip', 'from', 'Gary', 'Crum', '(', 'crum', '@', 'fcom.cc.utah.edu', ')', 'I', 'got', 'on', 'the', 'Phone', 'with', '``', 'Pontiac', 'Systems', \"''\", 'or', '``', 'Pontaic', 'Customer', 'Service', \"''\", 'or', 'whatever', ',', 'and', 'inquired', 'about', 'a', 'rumoured', 'Production', 'Hold', 'on', 'the', 'Formula', 'Firebird', 'and', 'Trans', 'Am', '.']\n",
      "\n",
      "표제어 추출 후 : ['After', 'a', 'tip', 'from', 'Gary', 'Crum', '(', 'crum', '@', 'fcom.cc.utah.edu', ')', 'I', 'got', 'on', 'the', 'Phone', 'with', '``', 'Pontiac', 'Systems', \"''\", 'or', '``', 'Pontaic', 'Customer', 'Service', \"''\", 'or', 'whatever', ',', 'and', 'inquired', 'about', 'a', 'rumoured', 'Production', 'Hold', 'on', 'the', 'Formula', 'Firebird', 'and', 'Trans', 'Am', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('표제어 추출 전 :',total_token[0])\n",
    "print()\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in total_token[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6100beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "die\n",
      "watch\n"
     ]
    }
   ],
   "source": [
    "# 단어의 품사가 무엇인지 알려줌\n",
    "print(lemmatizer.lemmatize('dies', 'v'))\n",
    "print(lemmatizer.lemmatize('watched', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2e742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 어간 추출 ]\n",
    "# 간 추출을 수행한 결과는 품사 정보가 보존되지 않습니다.\n",
    "# - 더 정확히는 어간 추출을 한 결과는 '사전에 존재하지 않는 단어'일 경우가 많습니다.\n",
    "# - 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업\n",
    "\n",
    "# 어간 추출 알고리즘 중 하나인 '포터 알고리즘(Porter Algorithm)'\n",
    "# - ex) 아래 예시\n",
    "\n",
    "# 어간 추출 속도는 표제어 추출보다 일반적으로 빠른데,\n",
    "# - 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로\n",
    "# - 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c465799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print()\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db798125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2) 대, 소문자 통합\n",
    "# - 물론 대문자와 소문자를 무작정 통합해서는 안됨\n",
    "# - 대문자와 소문자가 구분되어야 하는 경우도 있음\n",
    "# - 결국에는 예외 사항을 크게 고려하지 않고,\n",
    "#  => 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 함 (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7faf5fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdff77b",
   "metadata": {},
   "source": [
    "## 4) 정수 인코딩(Integer Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4520f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴퓨터는 텍스트보다 숫자를 더 잘 처리\n",
    "# 자연어 처리 과정에서 텍스트 -> 숫자로 바꾸는 기법의 첫 단계 => 각 단어를 정수에 매핑(mapping) 시키기\n",
    "# - 왜 이러한 작업이 필요한 지에 대해서는 뒤에서 원-핫 인코딩 실습이나, 워드 임베딩 챕터 등에서 알아보기로\n",
    "# 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a7a961",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb29725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) dictionary 사용하기\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# raw_text = \"A barber is a person. a barber is good person. a barber is huge person. \\\n",
    "# he Knew A Secret! The Secret He Kept is huge secret. Huge secret. \\\n",
    "# His barber kept his word. a barber kept his word. His barber kept his secret. \\\n",
    "# But keeping and keeping such a huge secret to himself was driving the barber crazy. \\\n",
    "# the barber went up a huge mountain.\"\n",
    "\n",
    "raw_text = '''A barber is a person. a barber is good person. a barber is huge person. \n",
    "he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. \n",
    "a barber kept his word. His barber kept his secret. \n",
    "But keeping and keeping such a huge secret to himself was driving the barber crazy. \n",
    "the barber went up a huge mountain.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2775914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ddae7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 텍스트 데이터가 문장 단위로 토큰화 된 것을 확인\n",
    "# -> 이제 정제 작업과 정규화 작업을 병행하며, 단어 토큰화를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69139e82",
   "metadata": {},
   "source": [
    "#### *** 문장 토큰화 -> 정제 & 정규화 -> 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8c9a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 단어들을 소문자화하여 단어의 개수를 통일\n",
    "# - 불용어와 단어 길이가 2이하인 경우에 대해서 단어를 일부 제외\n",
    "\n",
    "# 텍스트를 수치화하는 단계라는 것은 본격적으로 자연어 처리 작업에 들어간다는 의미!!\n",
    "# => 단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다.\n",
    "\n",
    "# 등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높기 때문에\n",
    "# ex) 빈도수가 1인 단어들은 전부 제외\n",
    "# word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장됨\n",
    "# word_to_index를 사용 -> 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# 예를 들어 sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩합니다.\n",
    "# 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있습니다.\n",
    "# -> 이처럼 단어 집합에 존재하지 않는 단어들이 생기는 상황을 Out-Of-Vocabulary(단어 집합에 없는 단어) 문제라고 합니다.\n",
    "# -> 약자로 'OOV 문제'라고도 합니다. word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩하겠습니다.\n",
    "# -> word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# => 이런걸 해주는 keras 모듈이 있으니 상황에 따라서 그걸 써도 편리 (해당 설명은 파이썬)\n",
    "# => 즉, Counter, FreqDist, enumerate를 사용하거나, 케라스 토크나이저를 사용하는 것을 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bc5653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 2) Counter 사용하기\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "328d1a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# 현재 sentences는 단어 토큰화가 된 결과가 저장되어져 있음\n",
    "# - 단어 집합(vocabulary)을 만들기 위해서 sentences에서 문장의 경계인 [, ]를 제거하고 단어들을 하나의 리스트로 만듦\n",
    "\n",
    "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4414cae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# 이를 파이썬의 Counter()의 입력으로 사용하면 '중복을 제거'하고 단어의 빈도수를 기록\n",
    "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a35acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴\n",
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f620bf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# barber란 단어가 총 8번 등장하였습니다. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴합니다.\n",
    "# -> 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있음\n",
    "# -> 등장 빈도수 상위 5개의 단어만 단어 집합으로 저장\n",
    "\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4bc681c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
    "\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc62946",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44c4dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) NLTK의 FreqDist 사용하기\n",
    "\n",
    "# NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원함\n",
    "# 위에서 사용한 Counter()랑 같은 방법으로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53ae1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "# np.hstack으로 문장 구분을 제거\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57b5148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있음\n",
    "# -> vocab에 단어를 입력하면 빈도수를 리턴\n",
    "\n",
    "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f70a5899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "# most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴\n",
    "# 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있음\n",
    "# 등장 빈도수 상위 5개의 단어만 단어 집합으로 저장\n",
    "\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c09bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "#앞서 Counter()를 사용했을 때와 결과 같음\n",
    "# 이전 실습들과 마찬가지로 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
    "\n",
    "# 그런데 이번에는 enumerate()를 사용하여 좀 더 짧은 코드로 인덱스를 부여\n",
    "\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60929448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 부여할 때는 enumerate()를 사용하는 것이 편리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf91831",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c39d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "# (+a) enumerate 이해하기\n",
    "\n",
    "# enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴함\n",
    "\n",
    "test_input = ['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
    "    print(\"value : {}, index: {}\".format(value, index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9e316",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "114038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 케라스(Keras)의 텍스트 전처리\n",
    "# 정수 인코딩을 위해서 케라스의 전처리 도구인 토크나이저를 사용\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# fit_on_texts는 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여하는데,\n",
    "# 정확히 앞서 설명한 정수 인코딩 작업이 이루어진다고 보면됨\n",
    "\n",
    "# [ word_index ] : 인덱스 부여\n",
    "# [ word_counts ] : 단어 카운팅\n",
    "# [ texts_to_sequences() ] : 코퍼스의 단어를 인덱스로 변환\n",
    "# tokenizer = Tokenizer(num_words=숫자) : 특정 상위 빈도수까지의 단어만 사용한다고 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49efaac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# (1) 각 단어에 인덱스 부여 => [ word_index ]\n",
    "# - 각 단어의 빈도수가 높은 순서대로 인덱스가 부여된 것을 확인\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58349c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# (2) 각 단어의 카운팅 => [ word_counts ]\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f83f40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'kept': 5, 'person': 6, 'word': 7, 'keeping': 8, 'good': 9, 'knew': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n",
      "\n",
      "[2, 6] : ['barber', 'person']\n",
      "[2, 1, 6] : ['barber', 'good', 'person']\n",
      "[2, 4, 6] : ['barber', 'huge', 'person']\n",
      "[1, 3] : ['knew', 'secret']\n",
      "[3, 5, 4, 3] : ['secret', 'kept', 'huge', 'secret']\n",
      "[4, 3] : ['huge', 'secret']\n",
      "[2, 5, 1] : ['barber', 'kept', 'word']\n",
      "[2, 5, 1] : ['barber', 'kept', 'word']\n",
      "[2, 5, 3] : ['barber', 'kept', 'secret']\n",
      "[1, 1, 4, 3, 1, 2, 1] : ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy']\n",
      "[2, 1, 4, 1] : ['barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# (3) 입력된 코퍼스를 단어 인덱스로 변환 => [ texts_to_sequences() ]\n",
    "print(f'{tokenizer.word_index}\\n')\n",
    "for i in range(len(preprocessed_sentences)):\n",
    "    print(f\"{tokenizer.texts_to_sequences(preprocessed_sentences)[i]} : {preprocessed_sentences[i]}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c352ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) 빈도수가 높은 상위 몇 개의 단어만 사용하겠다고 지정 => tokenizer = Tokenizer(num_words=숫자)\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# num_words에서 +1을 더해서 값을 넣어주는 이유\n",
    "# -> num_words는 숫자를 0부터 카운트하기 때문에.\n",
    "# -> 만약 5를 넣으면 0 ~ 4번 단어 보존을 의미하게 되므로\n",
    "# -> 뒤의 실습에서 1번 단어부터 4번 단어만 남게됨. 1 ~ 5번 단어까지 사용하고 싶다면 '+1'을 해줘야 하는 것임\n",
    "\n",
    "# 실질적으로 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어 집합의 크기로 산정하는 이유\n",
    "# -> 자연어 처리에서 패딩(padding)이라는 작업 때문\n",
    "# -> 일단, 숫자 0도 단어 집합의 크기로 고려해야한다고만 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "461aa7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)\n",
    "print()\n",
    "print(tokenizer.word_counts)\n",
    "# 상위 5개의 단어만 사용하겠다고 선언하였는데 여전히 13개의 단어가 모두 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9261db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# 사실 실제 적용은 texts_to_sequences를 사용할 때만 적용\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))\n",
    "\n",
    "# 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환하는데,\n",
    "# 상위 5개의 단어만을 사용하겠다고 지정하였으므로 1번 단어부터 5번 단어까지만 보존되고 나머지 단어들은 제거됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b481dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (+a) 경험상, 그다지 필요없다고 하긴함\n",
    "# 만약 word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면 아래의 코드도 방법\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# vocab_size = 5\n",
    "# words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] \n",
    "\n",
    "# # 인덱스가 5 초과인 단어 제거\n",
    "# for word in words_frequency:\n",
    "#     del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
    "#     del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "# print(tokenizer.word_counts)\n",
    "# print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bd308075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 OOV의 인덱스 : 1\n"
     ]
    }
   ],
   "source": [
    "# oov_token활용\n",
    "# - 케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징이 있음\n",
    "# - 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용\n",
    "\n",
    "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "\n",
    "# 만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다.\n",
    "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6b7da7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'kept': 5, 'person': 6, 'word': 7, 'keeping': 8, 'good': 9, 'knew': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n",
      "\n",
      "[2, 6] : ['barber', 'person']\n",
      "[2, 1, 6] : ['barber', 'good', 'person']\n",
      "[2, 4, 6] : ['barber', 'huge', 'person']\n",
      "[1, 3] : ['knew', 'secret']\n",
      "[3, 5, 4, 3] : ['secret', 'kept', 'huge', 'secret']\n",
      "[4, 3] : ['huge', 'secret']\n",
      "[2, 5, 1] : ['barber', 'kept', 'word']\n",
      "[2, 5, 1] : ['barber', 'kept', 'word']\n",
      "[2, 5, 3] : ['barber', 'kept', 'secret']\n",
      "[1, 1, 4, 3, 1, 2, 1] : ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy']\n",
      "[2, 1, 4, 1] : ['barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스에 대해서 정수 인코딩을 진행\n",
    "print(f'{tokenizer.word_index}\\n')\n",
    "for i in range(len(preprocessed_sentences)):\n",
    "    print(f\"{tokenizer.texts_to_sequences(preprocessed_sentences)[i]} : {preprocessed_sentences[i]}\", sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "18ababec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며,\n",
    "# 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 'OOV'의 인덱스인 1로 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382a01e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "412baa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 수정해서 필요한 부분까지만 사용\n",
    "# - 활용o : 불용어 제거, 단어 길이 2이하 제거\n",
    "# - 활용x : 빈도수 표시하는 것은 자동화해서 만들어주는 모듈로\n",
    "\n",
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거한다.\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다.\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0 \n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1c83e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e77a1",
   "metadata": {},
   "source": [
    "## 5) 패딩(Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 처리를 하다보면, 각 문장(or 문서)은 서로 길이가 다를 수 있음\n",
    "# but, 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고,\n",
    "# 한꺼번에 묶어서 처리할 수 있으니 병렬 연산을 위해서!!!\n",
    "# => 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e09570",
   "metadata": {},
   "source": [
    "#### (1) Numpy로 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5562f3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 텍스트\n",
    "preprocessed_sentences\n",
    "\n",
    "# 단어 집합을 만들고, 정수 인코딩을 수행\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c060de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 7\n"
     ]
    }
   ],
   "source": [
    "# 모두 동일한 길이로 맞춰주기 위해서,\n",
    "# 이 중에서 가장 길이가 긴 문장의 길이를 계산\n",
    "\n",
    "max_len = max(len(item) for item in encoded)\n",
    "print('최대 길이 :',max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "59c3b078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 길이가 긴 문장의 길이는 7\n",
    "# -> 모든 문장의 길이를 7로 맞춰주겠음\n",
    "\n",
    "# 이때 가상의 단어 'PAD'를 사용.\n",
    "# 'PAD'라는 단어가 있다고 가정, 이 단어는 0번 단어라고 정의\n",
    "# 길이가 7보다 짧은 문장에는 숫자 0을 채워서 길이 7로 맞춰줌\n",
    "\n",
    "for sentence in encoded:\n",
    "    while len(sentence) < max_len:\n",
    "        sentence.append(0)\n",
    "\n",
    "padded_np = np.array(encoded)\n",
    "padded_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5c74d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이들을 하나의 행렬로 보고, 병렬 처리를 할 수 있음.\n",
    "# 또한, 0번 단어는 사실 아무런 의미도 없는 단어이기 때문에\n",
    "# 자연어 처리하는 과정에서 기계는 0번 단어를 무시하게 될 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩(padding) : 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것\n",
    "# - 제로 패딩(zero padding) : 숫자 0을 사용하면"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c08913",
   "metadata": {},
   "source": [
    "#### (2) 케라스 전처리 도구로 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "983e3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequences() : 케라스에서는 제공하는 패딩 기능\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "aca0631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# encoded 값이 위에서 이미 패딩 후의 결과로 저장되었기 때문에\n",
    "# 패딩 이전의 값으로 다시 되돌리자\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bb98c868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  0,  1,  8,  5],\n",
       "       [ 0,  0,  0,  0,  1,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  2,  4,  3,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  2],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  2],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 0,  0,  0,  1, 12,  3, 13]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스로 패딩 수행 (앞0)\n",
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "edae0dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy로 패딩을 진행하였을 때와는 패딩 결과가 다름\n",
    "# -> pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라\n",
    "# -> 앞에 0으로 채우기 때문.\n",
    "\n",
    "# 뒤에 0을 채우고 싶다면, 인자로 padding='post'를 주면됨\n",
    "padded = pad_sequences(encoded, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a5cd35d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy를 이용하여 패딩을 했을 때와 결과가 동일.\n",
    "# 제로 결과가 동일한지 두 결과를 비교.\n",
    "(padded == padded_np).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eff56b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0],\n",
       "       [ 3,  2,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0],\n",
       "       [ 3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 긴 길이를 가진 문서의 길이를 기준으로 패딩을 한다고 가정하였지만,\n",
    "# 실제로는 꼭 가장 긴 문서의 길이를 기준으로 해야하는 것은 아님.\n",
    "\n",
    "# 길이에 제한을 두고 패딩할 수 있음.\n",
    "# [ maxlen ] 인자로 정수를 주면,\n",
    "# 해당 정수로 모든 문서의 길이를 동일하게 함.\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 5보다 짧은 문서들은 0으로 패딩되고,\n",
    "# 기존에 5보다 길었다면 데이터가 손실\n",
    "\n",
    "# ex) [ 7,  7,  3,  2, 10,  1, 11] -> [ 3,  2, 10,  1, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bc19b413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0],\n",
       "       [ 3,  2,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10],\n",
       "       [ 1, 12,  3, 13,  0]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터가 손실될 경우에 앞의 단어가 아니라 뒤의 단어가 삭제되도록 하고싶다면,\n",
    "# truncating이라는 인자를 사용.\n",
    "# truncating='post'를 사용할 경우 뒤의 단어가 삭제.\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)\n",
    "padded\n",
    "\n",
    "# ex) [ 7,  7,  3,  2, 10,  1, 11] -> [ 7,  7,  3,  2, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cc520650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# 숫자 0으로 패딩하는 것은 널리 퍼진 관례이긴 하지만,\n",
    "# 반드시 지켜야하는 규칙은 아님.\n",
    "\n",
    "# 만약, 숫자 0이 아니라 다른 숫자를 패딩을 위한 숫자로 사용하고 싶다면\n",
    "# 이 또한 가능합니다. 현재 사용된 정수들과 겹치지 않도록,\n",
    "# 단어 집합의 크기에 +1을 한 숫자로 사용\n",
    "\n",
    "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 크기보다 1 큰 숫자를 사용\n",
    "print(last_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "861e923a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5, 14, 14, 14, 14, 14],\n",
       "       [ 1,  8,  5, 14, 14, 14, 14],\n",
       "       [ 1,  3,  5, 14, 14, 14, 14],\n",
       "       [ 9,  2, 14, 14, 14, 14, 14],\n",
       "       [ 2,  4,  3,  2, 14, 14, 14],\n",
       "       [ 3,  2, 14, 14, 14, 14, 14],\n",
       "       [ 1,  4,  6, 14, 14, 14, 14],\n",
       "       [ 1,  4,  6, 14, 14, 14, 14],\n",
       "       [ 1,  4,  2, 14, 14, 14, 14],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13, 14, 14, 14]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 단어가 총 13개이고, 1번부터 13번까지 정수가 사용되었으므로\n",
    "# 단어 집합의 크기에 +1을 하면 마지막 숫자인 13보다 1이 큰 14를 얻습니다.\n",
    "# pad_sequences의 인자로 value를 사용하면 0이 아닌 다른 숫자로 패딩이 가능\n",
    "\n",
    "padded = pad_sequences(encoded, padding='post', value=last_value)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17844815",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18947f11",
   "metadata": {},
   "source": [
    "## 6) 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746b811",
   "metadata": {},
   "source": [
    "### 6-1) 원-핫 인코딩이란"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩에 대해서 배우기에 앞서 단어 집합(vocabulary) 에 대해서 정의해보자\n",
    "# - 단어 집합 : 텍스트의 모든 단어를 중복을 허용하지 않고 모은 것(서로 다른 단어들의 집합)\n",
    "# 원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합을 만드는 일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "16f4c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 고유한 정수 인덱스를 부여하였다고 합시다. 이제 이 숫자로 바뀐 단어들을 벡터로 다루고 싶다면?\n",
    "\n",
    "# 원-핫 인코딩(One-Hot Encoding) : 단어 집합의 크기를 벡터의 차원으로 하고,\n",
    "#                                 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고,\n",
    "#                                 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식\n",
    "\n",
    "# - 문자를 숫자로 바꾸는 여러가지 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법\n",
    "# - 머신 러닝, 딥 러닝을 하기 위해서는 반드시 배워야 하는 표현 방법\n",
    "\n",
    "# 원-핫 벡터(One-Hot vector) : 원-핫 인코딩으로 표현된 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd450f5c",
   "metadata": {},
   "source": [
    "### 6-2) 원-핫 인코딩(One-Hot Encoding) _ 파이썬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35612512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# < 원-핫 인코딩을 두 가지 과정 >\n",
    "# 첫째, 정수 인코딩을 수행 => 각 단어에 고유한 정수를 부여\n",
    "# 둘째, 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고,\n",
    "# - 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2f55df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "# (1) 토큰화 수행 (feat. Okt 형태소 분석기)\n",
    "from konlpy.tag import Okt  \n",
    "\n",
    "okt = Okt()  \n",
    "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0fd6b5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "# (2) 각 토큰에 대해서 고유한 정수 부여\n",
    "# - 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많음 (지금은 문장이 짧아서 빈도 고려x)\n",
    "\n",
    "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
    "print('단어 집합 :',word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "33fed5c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수\n",
    "def one_hot_encoding(word, word_to_index):\n",
    "    one_hot_vector = [0]*(len(word_to_index))\n",
    "    index = word_to_index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "one_hot_encoding(\"자연어\", word_to_index)\n",
    "# '자연어'는 정수 2이므로 원-핫 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2324fa8",
   "metadata": {},
   "source": [
    "### 6-3) 원-핫 인코딩(One-Hot Encoding) _ 케라스(Keras) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "31697423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "# 케라스는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원\n",
    "# -> 케라스만으로 정수 인코딩과 원-핫 인코딩을 순차적으로 진행\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 케라스 토크나이저를 이용한 정수 인코딩\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "print('단어 집합 :',tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1179c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면,\n",
    "# [ texts_to_sequences() ]를 통해서 이를 정수 시퀀스로 변환가능\n",
    "\n",
    "# -> 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인\n",
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "afeb91fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 해당 결과를 가지고, 원-핫 인코딩을 진행\n",
    "# 케라스는 정수 인코딩 된 결과로부터 원-핫 인코딩을 수행하는 [ to_categorical() ]를 지원\n",
    "\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0897dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"라는 문장이 [2, 5, 1, 6, 3, 7]로 '정수 인코딩'이 되고나서,\n",
    "# 각각의 인코딩 된 결과를 인덱스로 '원-핫 인코딩'이 수행된 모습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752f712",
   "metadata": {},
   "source": [
    "### 6-3) 원-핫 인코딩(One-Hot Encoding)의 한계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488cdec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단점\n",
    "# (1) 벡터의 차원이 늘어남\n",
    "# - 단어의 개수가 늘어날수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어남\n",
    "# - 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됨\n",
    "# - 1000개의 코퍼스가 있는 경우, 각 1개의 코퍼스는 1(1개), 0(999개)를 갖게 됨 => 저장 공간 측면에서 비효율적\n",
    "\n",
    "# (2) 단어의 유사도를 표현하지 못함 => 검색 시스템 등에서는 문제가 될 소지가 있음\n",
    "# 예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서\n",
    "# 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]\n",
    "# -> 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없음\n",
    "\n",
    "# '종로 숙소' 로 검색했을 경우, [ '종로 게하', '종로 호텔' ]등은 볼 수 없다는 말\n",
    "# - 연관검색어를 볼 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b202ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이러한 단점( (1), (2) )을 해결하기 위해 단어의 잠재 의미를 반영하여\n",
    "# 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있습니다.\n",
    "\n",
    "# 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있음\n",
    "# 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있음\n",
    "# + 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 있음\n",
    "\n",
    "# => 대부분은 워드 임베딩 챕터에서 다루게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192558c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e0671",
   "metadata": {},
   "source": [
    "## 7) 데이터의 분리(Splitting Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c71cf044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도 학습을 위한 데이터 분리 작업\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6001cb8",
   "metadata": {},
   "source": [
    "### (1) 지도 학습(Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f3106786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 답과 실제 정답인 y_test를 비교하면서\n",
    "# 기계가 정답을 얼마나 맞췄는지를 평가함\n",
    "# => 이 수치가 기계의 [ 정확도(Accuracy) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b7c39",
   "metadata": {},
   "source": [
    "### (2) X와 y분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0f23dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ('a', 'b', 'c')\n",
      "y 데이터 : (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2-1) [ zip 함수 ]\n",
    "# zip()함수 : '동일한 개수'를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할\n",
    "# -리스트의 리스트 구성에서 zip 함수는 X와 y를 분리하는데 유용\n",
    "\n",
    "X, y = zip(['a', 1], ['b', 2], ['c', 3])\n",
    "print('X 데이터 :',X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "439ed468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ('a', 'b', 'c')\n",
      "y 데이터 : (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 리스트의 리스트 또는 행렬 또는 뒤에서 배울 개념인 2D 텐서.\n",
    "sequences = [['a', 1], ['b', 2], ['c', 3]]\n",
    "X, y = zip(*sequences)\n",
    "print('X 데이터 :',X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "382e9c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>메일 본문</th>\n",
       "      <th>스팸 메일 유무</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>당신에게 드리는 마지막 혜택!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>내일 뵐 수 있을지 확인 부탁드...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>도연씨. 잘 지내시죠? 오랜만입...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(광고) AI로 주가를 예측할 수 있다!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    메일 본문  스팸 메일 유무\n",
       "0        당신에게 드리는 마지막 혜택!         1\n",
       "1    내일 뵐 수 있을지 확인 부탁드...         0\n",
       "2    도연씨. 잘 지내시죠? 오랜만입...         0\n",
       "3  (광고) AI로 주가를 예측할 수 있다!         1"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-2) [ 데이터프레임 ]\n",
    "\n",
    "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
    "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
    "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
    "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
    "columns = ['메일 본문', '스팸 메일 유무']\n",
    "\n",
    "df = pd.DataFrame(values, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3e205f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 : ['당신에게 드리는 마지막 혜택!', '내일 뵐 수 있을지 확인 부탁드...', '도연씨. 잘 지내시죠? 오랜만입...', '(광고) AI로 주가를 예측할 수 있다!']\n",
      "y 데이터 : [1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "X = df['메일 본문']\n",
    "y = df['스팸 메일 유무']\n",
    "\n",
    "print('X 데이터 :',X.to_list())\n",
    "print('y 데이터 :',y.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e0882c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# 2-3) [ Numpy ]\n",
    "# 임의의 데이터를 만들어서 Numpy의 슬라이싱(slicing)을 사용하여 데이터를 분리\n",
    "\n",
    "np_array = np.arange(0,16).reshape((4,4))\n",
    "print('전체 데이터 :')\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "144c6e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 데이터 :\n",
      "[[ 0  1  2]\n",
      " [ 4  5  6]\n",
      " [ 8  9 10]\n",
      " [12 13 14]]\n",
      "y 데이터 : [ 3  7 11 15]\n"
     ]
    }
   ],
   "source": [
    "# 마지막 열을 제외하고 X데이터에 저장합니다. 마지막 열만을 y데이터에 저장\n",
    "X = np_array[:, :3]\n",
    "y = np_array[:,3]\n",
    "\n",
    "print('X 데이터 :')\n",
    "print(X)\n",
    "print('y 데이터 :',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780def9",
   "metadata": {},
   "source": [
    "### (3) 테스트 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a3a77ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 전체 데이터 :\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "y 전체 데이터 :\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 3-1) [ train_test_split() ] => 사이킷 런을 이용\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)\n",
    "\n",
    "# X : 독립 변수 데이터. (배열이나 데이터프레임)\n",
    "# y : 종속 변수 데이터. 레이블 데이터.\n",
    "# test_size : 테스트용 데이터 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\n",
    "# train_size : 학습용 데이터의 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.\n",
    "# random_state : 난수 시드\n",
    "# -> train_size와 test_size는 둘 중 하나만 기재해도 됨\n",
    "\n",
    "# 임의로 X와 y 데이터를 생성\n",
    "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
    "\n",
    "print('X 전체 데이터 :')\n",
    "print(X)\n",
    "print('y 전체 데이터 :')\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8de7b715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 훈련 데이터 :\n",
      "[[2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "X 테스트 데이터 :\n",
      "[[8 9]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# train_test_split()은 기본적으로 데이터의 순서를 섞고나서 훈련 데이터와 테스트 데이터를 분리\n",
    "\n",
    "# - random_state의 값을 특정 숫자로 기재해준 뒤에 다음에도 동일한 숫자로 기재해주면,\n",
    "# - 항상 동일한 훈련 데이터와 테스트 데이터를 얻을 수 있음\n",
    "# -값을 변경하면 다른 순서로 섞인 채 분리되므로 이전과 다른 훈련 데이터와 테스트 데이터를 얻음\n",
    "\n",
    "# 7:3의 비율로 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "print('X 훈련 데이터 :')\n",
    "print(X_train)\n",
    "print('X 테스트 데이터 :')\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "40cc0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 훈련 데이터 :\n",
      "[1, 2, 3]\n",
      "y 테스트 데이터 :\n",
      "[4, 0]\n"
     ]
    }
   ],
   "source": [
    "print('y 훈련 데이터 :')\n",
    "print(y_train)\n",
    "print('y 테스트 데이터 :')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2af03f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 훈련 데이터 :\n",
      "[4, 0, 3]\n",
      "y 테스트 데이터 :\n",
      "[2, 1]\n"
     ]
    }
   ],
   "source": [
    "# random_state의 값을 변경\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "print('y 훈련 데이터 :')\n",
    "print(y_train)\n",
    "print('y 테스트 데이터 :')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f3d4106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state 값이 1234일 때와 전혀 다른 y데이터가 출력\n",
    "# -> 데이터가 다른 순서로 섞였다는 의미\n",
    "\n",
    "# random_state의 값을 고정해두면 실행할 때마다 항상 동일한 순서로 데이터를 섞으므로,\n",
    "# 동일한 코드를 다음에 재현하고자 할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "98b612b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2) 수동으로 분리하기\n",
    "# train_test_split()과 다른 점은 데이터가 섞이지 않은 채 어느 지점에서 데이터를 앞과 뒤로 분리했다는 점입니다.\n",
    "# 만약, 수동으로 분리하게 된다면 데이터를 분리하기 전에 수동으로 데이터를 섞는 과정이 필요할 수 있습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28df0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e1acc",
   "metadata": {},
   "source": [
    "### 8) 한국어 전처리 패키지\n",
    "---\n",
    "https://wikidocs.net/92961"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
